\documentclass[12pt]{report} % Increased the font size to 12pt
\usepackage{epigraph}
\usepackage{geometry}

% Optional: customize the style of epigraphs
\setlength{\epigraphwidth}{0.5\textwidth} % Adjust the width of the epigraph
\renewcommand{\epigraphflush}{flushright} % Align the epigraph to the right
\renewcommand{\epigraphrule}{0pt} % No horizontal rule
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage{hyperref} % Added for hyperlinks
\usepackage{listings} % Added for code listings
\usepackage{color}    % Added for color definitions
\usepackage[super]{nth}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{cite}
\usepackage{algpseudocode}
\usetikzlibrary{shapes.geometric, arrows, positioning}

\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

% Define the graphics path
%\graphicspath{{./Plots/}}

% Define the header and footer for general pages
\pagestyle{fancy}
\fancyhf{} % Clear all header and footer fields
\fancyhead{} % Initially, the header is empty
\fancyfoot[C]{\thepage} % Page number at the center of the footer
\renewcommand{\headrulewidth}{0pt} % No header line on the first page of chapters
\renewcommand{\footrulewidth}{0pt} % No footer line

% Define the plain page style for chapter starting pages
\fancypagestyle{plain}{%
  \fancyhf{} % Clear all header and footer fields
  \fancyfoot[C]{\thepage} % Page number at the center of the footer
  \renewcommand{\headrulewidth}{0pt} % No header line
}

% Apply the 'fancy' style to subsequent pages in a chapter
\renewcommand{\chaptermark}[1]{%
  \markboth{\MakeUppercase{#1}}{}%
}

% Redefine the 'plain' style for the first page of chapters
\fancypagestyle{plain}{%
  \fancyhf{}%
  \fancyfoot[C]{\thepage}%
  \renewcommand{\headrulewidth}{0pt}%
}

% Header settings for normal pages (not the first page of a chapter)
\fancyhead[L]{\slshape \nouppercase{\leftmark}} % Chapter title in the header
\renewcommand{\headrulewidth}{0.4pt} % Header line width on normal pages

\setlength{\headheight}{14.49998pt}
\addtolength{\topmargin}{-2.49998pt}
% Define colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup for code listings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

% Definition of the tcolorbox for definitions
\newtcolorbox{definitionbox}[1]{
  colback=red!5!white,
  colframe=red!75!black,
  colbacktitle=red!85!black,
  title=#1,
  fonttitle=\bfseries,
  enhanced,
}

% Definition of the tcolorbox for remarks
\newtcolorbox{remarkbox}{
  colback=blue!5!white,     % Light blue background
  colframe=blue!75!black,   % Darker blue frame
  colbacktitle=blue!85!black, % Even darker blue for the title background
  title=Remark:,            % Title text for remark box
  fonttitle=\bfseries,      % Bold title font
  enhanced,
}

% Definition of the tcolorbox for examples
\newtcolorbox{examplebox}{
  colback=green!5!white,
  colframe=green!75!black,
  colbacktitle=green!85!black,
  title=Example:,
  fonttitle=\bfseries,
  enhanced,
}

% Definitions and examples will be put in these environments
\newenvironment{definition}
    {\begin{definitionbox}}
    {\end{definitionbox}}

\newenvironment{example}
    {\begin{examplebox}}
    {\end{examplebox}}

\geometry{top=1.5in} % Adjust the value as needed
% ----------------------------------------------------------------------------------------


\title{M2 Applications of Machine Learning}
\author{CRSiD: tmb76}
\date{University of Cambridge}

\begin{document}

\maketitle

\tableofcontents

\chapter*{Using a Diffusion Model on the MNIST Dataset}

\chapter{Training a Diffusion Model}

\section{Denoising Diffusion Probabilistic Model (DDPM)}

\subsection{Diffusion Models}

Diffusion models are a class of probabilistic latent variable models. They consist of an encoder and decoder. The encoder takes the input data and maps it to a latent space in a series of steps, resulting in a series of intermediate latent vairables. The encoder is similar to variational autoencoders (VAEs) in that it maps the input data to a latent space. However, the particularity of the encoder here is that the mappings it will apply at each time step are predetermnied. The key part is the decoder which is trained to learn what is the reverse process of the encoder, therefore being then able to produce samples\cite[p.348]{prince2023understanding}.

\subsection{Denoising Diffusion Probabilistic Model (DDPM)}

In this report, the writing conventions of the Prince textbook will be followed\cite{prince2023understanding}. The model used for this project is a DDPM. For this mode, the encoder takes in input data $\mathbf{x}$ and maps it to a latent space $\mathbf{z_{T}}$, of the same dimensionality as $\mathbf{x}$, in a series of steps: $\mathbf{z_{0}} \rightarrow \mathbf{z_{1}} \rightarrow \ldots \rightarrow \mathbf{z_{T}}$. This is defined by a Markov Chain that is known, which at each step adds Gaussian Noise following a Noise or Variance Schedule, $\beta_{1, \dots, T}$. As it is a Markov Chain, and a type of variational autoencoder, the encoder can be described by an approximate probability distribution $q$ such that\cite{ho2020denoising}:

\begin{equation}
q(\mathbf{z}_{1:T}|\mathbf{x}) = \prod{t=1}{T}q(\mathbf{z}_{t}|\mathbf{z}_{1:t-1}))
\end{equation}

Where the individual step is given by:

\begin{equation}
  q(\mathbf{z}_{t}|\mathbf{z}_{t-1}) = \mathcal{N}(\mathbf{z}_{t}; \sqrt{1 - \beta_{t}}\mathbf{z}_{t-1}, \beta_{t}\mathbf{I})
\end{equation}

In other words, $\beta_{t}$ describes how much noise is going to be added to the input data at each step $t$. Prince's textbook also provides a closed form expression which shows this more clearly\cite{prince2023understanding}:

\begin{equation}
  \mathbf{z}_{1} = \sqrt{1 - \beta_{1}}\mathbf{x} + \sqrt{\beta_{1}}\mathbf{\epsilon}_{1}
\end{equation}

And it can be shown that after t steps, this gives:

\begin{equation}
  \mathbf{z}_{t} = \sqrt{\alpha_{t}}\mathbf{x} + \sqrt{1 - \alpha_{t}}\mathbf{\epsilon}
\end{equation}

where $\alpha_{t} = \prod_{s=1}^{t} 1 - \beta_{s}$ and $\mathbf{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$, is a sample from a standard normal distribution, and therefore is the actual noise added. The decoder is trained to learn the reverse process of the encoder, or simply how to go from $\mathbf{z}_{T}$ to $\mathbf{z}_{T-1}$, continuing back through the latent variables to the input data $\mathbf{x}$. Coming back to the approximate probability distribution $q$, the decoder is trained to learn the reverse distributions $q(\mathbf{z}_{t-1}|\mathbf{z}_{t})$. Approximating them as normal distributions, they can be written:

\begin{equation}
  Pr(\mathbf{z}_{t-1}|\mathbf{z}_{t}, \phi{t}) = \mathcal{N}_{z_{t-1}}(\mathbf{f}_{t}[\mathbf{z}_{t}, \phi_{t}], \sigma_{t}^{2}\mathbf{I})
\end{equation}

where $\mathbf{f}_{t}$ is a neural network that takes $\mathbf{z}_{t}$ as input and has parameters $\phi_{t}$, which here is just the timestep $t$. The reason why the model predicts the mean of the normal distribution with the variance being fixed to $\sigma_{t}^{2}\mathbf{I}$ is discussed in greater detail in the Ho et al. (2020) paper\cite{ho2020denoising}. The training algorithm can then be written as follows:

\begin{definitionbox}{Training Algorithm for DDPM reverse process \cite{ho2020denoising}}
  \begin{algorithmic}
    \State \textbf{Input:} Data $\mathbf{x}$
    \State \textbf{Output:} $\mathbf{\mu_{t}} = \mathbf{f}_{t}[\mathbf{z}_{t},t]$
    \State \textbf{repeat}
      \For{$i \in \mathcal{B}$} \Comment{For each training example index in batch}
        \State $t \sim \mathcal{U}(1, \dots, T)$ \Comment{Sample a random time step}
        \State $\mathbf{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ \Comment{Sample noise}
        \State $L = ||\mathbf{\epsilon} - \mathbf{\epsilon}_{\phi}(\sqrt{\alpha_{t}}\mathbf{x} + \sqrt{1 - \alpha_{t}}\mathbf{\epsilon}, t)||^{2}$ \Comment{Compute individual noise}
      \EndFor \Comment{Accumulate losses for batch and take gradient descent step}
    \State \textbf{until} convergence
  \end{algorithmic}
\end{definitionbox}

Predicting the noise in the algorithm instead of the mean is done by modifying the parameterization of $\mathbf{f}_{t}[\mathbf{z}_{t}, t]$ \cite{ho2020denoising}.

\section{Training the Model on the MNIST Dataset}

Here, the model chosen to learn prediction of the noise is a Convolutional Neural Network (CNN). It is set to have a kernel size of 7. CNN's are often used in image data processing \cite[p.161]{prince2023understanding}, partly since they provide a way to reduce the number of weights and biases, which becomes an issue quickly in images as they are high dimensional inputs. More importantly, image recognition or prediction requires more a knowledge of what patterns define certain objects, and this whatever the position on the image. And this is something a fully connected neural network struggles with since it does not have any notion of spatial relationships between pixels, and would need to learn what a certain object looks in every rotation/position possible. This is key in the case of the MNIST dataset where only 9 objects are considered but they are found to be extremely variable, as they are handwritten.

The activation function used is GELU, which is a Gaussian Error Linear Unit, defined as:

\begin{equation}
  \text{GELU}(x) = x\Phi(x) = x \cdot \frac{1}{2} + [1 + \text{erf} ( \frac{x}{\sqrt{2}} )]
\end{equation}

where $\Phi(x) = P(X \leq x), X \sim \mathcal{N}(0, 1)$. It was introduced in 2016 by Hendrycks and Gimpel \cite{hendrycks2016gelu}, and introduces a mix of non-deterministic masking of inputs though still depending on the input values (see Figure \ref{fig:gelu}).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/GeLU_example.png}
  \caption{GELU activation function}
  \label{fig:gelu}
\end{figure}

The standard DDPM model was trained for 100 epochs on the MNIST dataset, with a batch size of 128. The model was trained using the Adam optimizer with a learning rate of $2 \times 10^{-4}$, and the loss function used was the mean squared error.

First, the loss at each iteration was obtained and plotted in Figure \ref{fig:loss}. As can be seen the loss does decrease over time, fast at first then much slower which is expected.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/losses_for_DDPM_default@100.png}
  \caption{Loss at each iteration of the training process, and the average loss over each epoch}
  \label{fig:loss}
\end{figure}

\newpage
One issue that arises in the context of the MNIST dataset is that the MSE is susceptible to be very low even though the samples generated are not good. This is because the MSE is purely looking at the problem quantitatively. To get a qualitative sense of the samples generated, 16 samples were generated for different epochs using the samping algorithm described in the Prince textbook\cite[p. 363]{prince2023understanding}, by giving the model a pure Gaussian noise input and letting it gradually denoise it. The samples generated at epoch 1, 20, 40 and 60 are shown in Figure \ref{fig:ddpm_samples}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.2\textwidth]{../contents/ddpm_sample_0001_default.png}
  \includegraphics[width=0.2\textwidth]{../contents/ddpm_sample_0020_default.png}
  \includegraphics[width=0.2\textwidth]{../contents/ddpm_sample_0040_default.png}
  \includegraphics[width=0.2\textwidth]{../contents/ddpm_sample_0060_default.png}
  \caption{Samples generated by the DDPM model at epochs 1, 20, 40 and 60}
  \label{fig:ddpm_samples}
\end{figure}

\newpage
As expected the samples generated at epoch 1 are very close to being just noise, though some samples show some patterns appearing. By epoch 20, the model is consistently generating symbols, with some ressembling numbers. And it then takes a longer time to get to a point where the samples are consistently numbers. This comes back to the discussion above, as the symbols still provide a low MSE, but are not good samples, and the model is purely based on MSE.

To quantitatively evaluate the quality of the samples, the Fr√©chet Inception Distance (FID) score was used. The FID score measures how similar two sets of images are by comparing the feature representations of the images\cite{fid}. The lower the score the better. The FID score was calculated for the samples generated at each epoch, and the results are shown in Figure \ref{fig:fid_ddpm}. A more robust estimate is computed once all epochs are run, on a larger sample of generated images.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/fid_for_DDPM_default.png}
  \caption{FID score at each epoch of the training process}
  \label{fig:fid_ddpm}
\end{figure}

As expected, the FID score decreases over time. However, these still show that the model is not generating good samples, as the FID score is still quite high. And even at the end of training for the last epoch, the FID score was ... .

Comparing to figures obtained in the Ho et al. (2020) paper\cite{ho2020denoising}, or the Bansal et al. (2022) paper\cite{bansal2022cold}, it can be seen that the score obtained here indicates quite bad performance. However, it is important to note that the model was trained for only 100 epochs, with quite a shallow CNN. Further, the FID here was used regardless of the digit, which may be different to in the papers mentionned. The rationale here is that the FID score being based on the distribution of computer vision features\cite{fid}, it should still be able to capture the quality of the samples generated when using it for all digits together. Finally, it is not a perfectly objective metric so it will, mainly be used for comparison of the different models trained in this report. In that aspect, it will be a more robust metric.

Additionally, the Inception Score (IS) was calculated for the samples generated at each epoch. The IS is a metric that measures the quality of generated images, by looking at the diversity and quality of the generated images. More specifically, and in this context, it will measure the variety of images/digits generated, but also how much each image looks like a digit. The IS score has lowest value 1.0 and the larger it is, the better.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/incept_score_for_DDPM_default.png}
  \caption{IS score at each epoch of the training process for the DDPM model}
  \label{fig:is_ddpm}
\end{figure}

Figure \ref{fig:is_ddpm} shows the IS score at each epoch of the training process. And the final epoch score was found to be: . Here, the IS score decreases over the epochs, which is not what is expected. Once again, this result has to be considered with caution, as because of computing efficiency, it is computed with a small sample of generated images. The more important point is that this metric only compares the generated images to themselves, so it is only a measure of the diversity and quality of the generated images.

\section{Running the Model for Different Hyperparameters}

For the previous trained mode, a set of hyperparameters were used (see Table \ref{tab:hyperparams}. The noise schedule $\beta$'s are set by a tuple, which sets the range of values that the noise can take. They are then defined as: $\beta_{t} = \frac{(\beta_{2}-\beta_{1}) \times t}{T + \beta_{1}} $, for $t = 0, \dots, T$. The number of timesteps is set to 1000, and the CNN was set to have 4 hidden layers with 16, 32, 32, and 16 hidden units respectively.

\begin{table}[h]
  \centering
  \begin{tabular}{c c}
    \hline
    Hyperparameter & Value/Choice \\
    \hline
    $\beta$'s & ($10^{-4}$, $0.02$) \\
    Number of timesteps & 1000 \\
    Learning Rate & $2 \times 10^{-4}$ \\
    Number of hidden layers \& units & ($16$, $32$, $32$, $16$) \\
    Batch Size & 128 \\
    Activation function & GELU \\
    \hline
  \end{tabular}
  \caption{Hyperparameters used for the training of the DDPM model}
  \label{tab:hyperparams}
\end{table}

In this section, another set of hyperparameters is chosen and the DDPM model is trained again with these hyperparameters. The new hyperparameters are shown in Table \ref{tab:hyperparams2}.

\begin{table}[h]
  \centering
  \begin{tabular}{c c}
    \hline
    Hyperparameter & Value/Choice \\
    \hline
    $\beta$'s & ($10^{-4}$, $0.02$) \\
    Number of timesteps & 100 \\
    Learning Rate & $6 \times 10^{-4}$ \\
    Number of hidden layers \& units & ($16$, $32$, $32$, $16$) \\
    Batch Size & 128 \\
    Activation function & SELU \\
    \hline
  \end{tabular}
  \caption{New hyperparameters used for the training of the DDPM model}
  \label{tab:hyperparams2}
\end{table}

By making the number of timesteps smaller, the model will have a "steeper" learning curve, as it will have to learn to denoise the image in fewer steps. The learning rate is increased to $6 \times 10^{-4}$ to encourage the model to learn faster. Now, whether this learning will be in the right direction is another question, but the idea is to see if the model can do well with a harder set of hyperparameters. Finally, the activation function is changed to SELU, which is a Scaled Exponential Linear Unit, which resembles the GELU function but without the "dip" below 0 (see Figure \ref{fig:gelu}). Overall, these hyperparameters are chosen to see if the model can perform as well with a harder set of hyperparameters, or if more capacity is needed if the model is to learn well.

Again, the model was trained for 100 epochs, and the loss at each iteration was obtained and plotted in Figure \ref{fig:loss2}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/losses_for_DDPM_testing.png}
  \caption{Loss at each iteration of the training process, and the average loss over each epoch}
  \label{fig:loss2}
\end{figure}


The same decrease of the loss can be seen here though it plateau's a higher. Looking at the samples generated at epochs 1, 20, 40 and 60, shown in Figure \ref{fig:ddpm_samples2}, it can be seen that the model is generating symbols at the same times as in the previous model, but there is no improvement towards those symbols becoming digits. Even at the latest epoch, the samples are still far from looking like digits (Fig \ref{fig:ddpm_samples3}).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.2\textwidth]{../contents/ddpm_sample_0000_testing.png}
  \includegraphics[width=0.2\textwidth]{../contents/ddpm_sample_0020_testing.png}
  \includegraphics[width=0.2\textwidth]{../contents/ddpm_sample_0040_testing.png}
  \includegraphics[width=0.2\textwidth]{../contents/ddpm_sample_0060_testing.png}
  \caption{Samples generated by the DDPM model at epochs 1, 20, 40 and 60}
  \label{fig:ddpm_samples2}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.4\textwidth]{../contents/ddpm_sample_0080_testing.png}
  \caption{Samples generated by the DDPM model at epoch 100}
  \label{fig:ddpm_samples3}
\end{figure}

The FID score at each epoch is shown in Figure \ref{fig:fid_ddpm2}. The FID score decreases over time, though it does end up higher than the previous model. The final FID score was found to be ... . The IS score at each epoch is shown in Figure \ref{fig:is_ddpm2}, and the final IS score was found to be ... .

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/fid_for_DDPM_testing.png}
  \caption{FID score at each epoch of the training process}
  \label{fig:fid_ddpm2}

  \includegraphics[width=0.8\textwidth]{../Plots/incept_score_for_DDPM_testing.png}
  \caption{IS score at each epoch of the training process for the DDPM model}
  \label{fig:is_ddpm2}
\end{figure}

One thing worth noting is that the FID shows consistency with the previous model run.



\chapter{Custom Degradation Function}

In Bansal et al. (2022, \cite{bansal2022cold}), a conceptual summary of degradation functions is given. Starting with image $\mathbf{x} \in \mathbb{R}$, the degradation of the image, or forward process of the encoder for the DDPM described in section 1.1.2, can be considered as follows: $\mathbf{x}_{t} = D(\mathbf{x}, t)$, where $D$ is the degradation operator and $t$ is the severity of the degradation. In other words, $D(\mathbf{x}, 0) = \mathbf{x}$. In Chapter 1, the $D$ operator consisted of adding Gaussian noise with variance described by the Variance/Noise Schedule $\beta_{1, \dots, T}$. In this chapter, a custom degradation function, or operator $D$, will be described and used to train the model on the MNIST dataset. The important part of the degradation function is that an inverse process, $R$, is required to invert $D$ and satisfies: $R(\mathbf{x}_t, t) \approx \mathbf{x}$, and $R(\mathbf{x}_t, 1) \approx \mathbf{x}_{t-1}$. The degradation function will be described in the next section. And as was discussed in Chapter 1, this is implemented through a neural network parameterized by $\phi$ and trained to minimize the loss $L = ||\mathbf{x} - R_{\phi}(D(\mathbf{x}, t), t)||$, taking an $l_{1}$ norm \cite{bansal2022cold}.

\section{A Row/Column Averaging degradation function}

Taking inspiration from the super-resolution degradation function described in Bansal et al. (2022) \cite{bansal2022cold}, a degradation function that averages the rows and columns of the image is proposed. An example of the degradation function for columns is described below:

\begin{definitionbox}{Column Averaging Degradation Function in the forward process}
  \begin{algorithmic}
    \State \textbf{Input:} Data $\mathbf{x}$
    \State \textbf{Output:} $\mathbf{x}_{0} = \mathbf{f}_{t}[\mathbf{z}_{t}, t]$
    \State \textbf{repeat}
      \For{$i \in \mathcal{B}$} \Comment{For each training example index in batch}
        \State $t \sim \mathcal{U}(1, \dots, T)$ \Comment{Sample a random time step}
        \For{$j \in \text{Column Schedule[1, \dots, t]}:$} \Comment{Degradation of columns}
          \State $\mathbf{z}_{t}[:, j] = \frac{1}{\text{28}}\sum_{k=1}^{28}\mathbf{x}[k, j]$
        \EndFor
        \State $L = ||\mathbf{x} - \mathbf{f}_{t}[\mathbf{z}_{t}, t]||^{2}$ \Comment{Predict the original image}
      \EndFor \Comment{Accumulate losses for batch and take gradient descent step}
    \State \textbf{until} convergence
  \end{algorithmic}
\end{definitionbox}

In this forward process the column or row schedule is defined in 3 ways:

- Randomly: an order of rows covering all values from 1 to 28 is randomly chosen

- Outside-In: [1,28,2,27,3,26,...]

- Inside-Out: [14,15,13,16,12,17,...]


The degradation function then, given a time step $t$, will average the first $t$ rows/columns listed in the schedule. The loss is then calculated as the MSE between the original image and the image predicted by the model from that time step, and the model is trained to minimize this loss.

For the inverse process, algorithm 2 of the Bansal et al. (2022) paper is used, as it gives better performance for cold diffusion methods\cite[p. 4]{bansal2022cold}. The inverse process is described as follows:

\begin{definitionbox}{Inverse Process for Column Averaging Degradation Function}
  \begin{algorithmic}
    \State \textbf{Input:} Degraded Data $\mathbf{z}_{T}$
    \State \textbf{Output:} Prediction of original sample $\mathbf{x}$
      \For{$s = T, T-1, \dots, 1$} \Comment{For each time step in reverse order}
        \State $\hat{\mathbf{x}} \gets \mathbf{f}_{T}[z_{T}, T]$ \Comment{Make a direct prediction}
        \State $\mathbf{x}_{s-1} = \mathbf{x}_{s} - \mathbf{D}(\hat{\mathbf{x}}, s) + \mathbf{D}(\hat{\mathbf{x}}, s-1)$ \Comment{Predict the previous time step's image}
      \EndFor
  \end{algorithmic}
\end{definitionbox}

Where $\mathbf{D}$ is the degradation function, so $\mathbf{D}(\hat{\mathbf{x}}, s)$ is the image $\hat{\mathbf{x}}$ degraded at time step $s$, with the first $s$ rows/columns in the schedule order averaged. The degradation was originally made to average rows/columns by groups of 4, resulting in a degraded image with 7 rows/columns. However, these led to too much information lost and more importantly only 7 time steps over which the model could learn how to de-average the rows/columns. With each row/column averaged on their own, this gives 28 time steps, which should allow the model to learn better how to de-average the rows/columns.


\section{Training the modified model on the MNIST dataset}


The non-grouped column averaging model is discussed here as it gave the best result. The model was trained for ... epochs, with the following hyperparameters:

\begin{table}[h]
  \centering
  \begin{tabular}{c c}
    \hline
    Hyperparameter & Value/Choice \\
    \hline
    Row/Column Schedule & Random \\
    Number of timesteps & 28 \\
    Learning Rate & $2 \times 10^{-4}$ \\
    Number of hidden layers \& units & ($16$, $32$, $32$, $16$) \\
    Batch Size & 128 \\
    Activation function & GELU \\
    \hline
  \end{tabular}
  \caption{Hyperparameters used for the training of the column averaging cold diffusion model}
  \label{tab:hyperparams3}
\end{table}

As before, samples are generated for epochs 1, 20, 40 and 60, and the results are shown in Figure \ref{fig:col_avg_samples}.  ... get numbers quickly but only certain ones. discussion about direction depemndent information. Model is therefore biased...

In terms of the numbers...The loss at each iteration was obtained and are plotted together in Figure \ref{fig:loss3}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/losses_for_custom_col_default_28.png}
  \caption{Loss at each iteration of the training process, and the average loss over each epoch}
  \label{fig:loss3}
\end{figure}

The FID score at each epoch is shown in Figure \ref{fig:fid_col_avg}, and the final FID score was found to be ... . The IS score at each epoch is shown in Figure \ref{fig:is_col_avg}, and the final IS score was found to be ... .

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{../Plots/fid_for_custom_col_default_28.png}
  \caption{FID score at each epoch of the training process}
  \label{fig:fid_col_avg}

  \includegraphics[width=0.8\textwidth]{../Plots/incept_score_for_custom_col_default_28.png}
  \caption{IS score at each epoch of the training process for the column averaging cold diffusion model}
  \label{fig:is_col_avg}
\end{figure}


\section{Comparing with the original model}

Evaluate the fidelity of the samples generated by the two models, discussing any differences between them.



\chapter{Appendix}

\section{Other versions of the degradation function}

Here is shown results obtained for the different versions of the degradation function.

plot of original, degraded, direct sampled for col7

final loss, FID, IS for col7

plot of original, degraded, direct sampled for row7

final loss, FID, IS for row7

plot of original, degraded, direct sampled for row28

final loss, FID, IS for row28

\section{AI tools use}

Ask chatgpt to summarise your prompts





\bibliographystyle{plain}
\bibliography{refs.bib}

\end{document}
