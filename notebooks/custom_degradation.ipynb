{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining a diffusion model with custom degradation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p contents_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/61058798/python-relative-import-in-jupyter-notebook\n",
    "import os, sys\n",
    "\n",
    "dir2 = os.path.abspath(\"\")\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "from src import funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(75016)\n",
    "np.random.seed(75016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedules(order: int, T: int, type: str) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Returns order and schedule for each row/column in the image.\"\"\"\n",
    "\n",
    "    assert order == 1 or order == 2 or order == 3, \"Order must be either 1 or 2\"\n",
    "\n",
    "    half_point = T // 2 - 1\n",
    "    backward = list(range(13, -1, -1))\n",
    "    forward = list(range(13 + 1, 28))\n",
    "    index = []\n",
    "    if order == 1:\n",
    "        for i in range(half_point + 1):\n",
    "            index.append(forward[i])\n",
    "            index.append(backward[i])\n",
    "        if T % 2 == 1:\n",
    "            index.append(forward[half_point + 1])\n",
    "    elif order == 2:\n",
    "        for i in range(1, half_point + 2):\n",
    "            index.append(backward[-i])\n",
    "            index.append(forward[-i])\n",
    "        if T % 2 == 1:\n",
    "            index.append(forward[-half_point - 2])\n",
    "    else:\n",
    "        # Unique random order\n",
    "        index = random.sample(range(28), 28)\n",
    "\n",
    "    return {type: torch.tensor(index)}\n",
    "\n",
    "\n",
    "def schedules_7(order: int, T: int, type: str) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Returns order and schedule for each 4 wide row in the image.\"\"\"\n",
    "\n",
    "    assert order == 1 or order == 2 or order == 3, \"Order must be either 1 or 2\"\n",
    "\n",
    "    half_point = T // 2 - 1\n",
    "    idx_groups = list(np.array_split(range(28), 7))\n",
    "    backward = list(range(3, -1, -1))\n",
    "    forward = list(range(4, 7))\n",
    "    index = []\n",
    "    if order == 1:\n",
    "        for i in range(half_point + 1):\n",
    "            index.append(idx_groups[backward[i]])\n",
    "            index.append(idx_groups[forward[i]])\n",
    "        if T % 2 == 1:\n",
    "            index.append(idx_groups[backward[half_point + 1]])\n",
    "    elif order == 2:\n",
    "        for i in range(1, half_point + 2):\n",
    "            index.append(idx_groups[forward[-i]])\n",
    "            index.append(idx_groups[backward[-i]])\n",
    "        if T % 2 == 1:\n",
    "            index.append(idx_groups[-half_point - 2])\n",
    "    else:\n",
    "        # Unique random order\n",
    "        idx = random.sample(range(7), 7)\n",
    "        index = [idx_groups[i] for i in idx]\n",
    "\n",
    "    return {type: torch.tensor(index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        *,\n",
    "        expected_shape,\n",
    "        act=nn.GELU,\n",
    "        kernel_size=7,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.LayerNorm((out_channels, *expected_shape)),\n",
    "            act(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        expected_shape=(28, 28),\n",
    "        n_hidden=(64, 128, 64),\n",
    "        kernel_size=7,\n",
    "        last_kernel_size=3,\n",
    "        time_embeddings=16,\n",
    "        act=nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        last = in_channels\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for hidden in n_hidden:\n",
    "            self.blocks.append(\n",
    "                CNNBlock(\n",
    "                    last,\n",
    "                    hidden,\n",
    "                    expected_shape=expected_shape,\n",
    "                    kernel_size=kernel_size,\n",
    "                    act=act,\n",
    "                )\n",
    "            )\n",
    "            last = hidden\n",
    "\n",
    "        # The final layer, we use a regular Conv2d to get the\n",
    "        # correct scale and shape (and avoid applying the activation)\n",
    "        self.blocks.append(\n",
    "            nn.Conv2d(\n",
    "                last,\n",
    "                in_channels,\n",
    "                last_kernel_size,\n",
    "                padding=last_kernel_size // 2,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ## This part is literally just to put the single scalar \"t\" into the CNN\n",
    "        ## in a nice, high-dimensional way:\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(time_embeddings * 2, 128),\n",
    "            act(),\n",
    "            nn.Linear(128, 128),\n",
    "            act(),\n",
    "            nn.Linear(128, 128),\n",
    "            act(),\n",
    "            nn.Linear(128, n_hidden[0]),\n",
    "        )\n",
    "        frequencies = torch.tensor(\n",
    "            [0] + [2 * np.pi * 1.5**i for i in range(time_embeddings - 1)]\n",
    "        )\n",
    "        self.register_buffer(\"frequencies\", frequencies)\n",
    "\n",
    "    def time_encoding(self, t: int) -> torch.Tensor:\n",
    "        phases = torch.concat(\n",
    "            (\n",
    "                torch.sin(t[:, None] * self.frequencies[None, :]),\n",
    "                torch.cos(t[:, None] * self.frequencies[None, :]) - 1,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return self.time_embed(phases)[:, :, None, None]\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        # Shapes of input:\n",
    "        #    x: (batch, chan, height, width)\n",
    "        #    t: (batch,)\n",
    "\n",
    "        embed = self.blocks[0](x)\n",
    "        # ^ (batch, n_hidden[0], height, width)\n",
    "\n",
    "        # Add information about time along the diffusion process\n",
    "        #  (Providing this information by superimposing in latent space)\n",
    "        embed += self.time_encoding(t)\n",
    "        #         ^ (batch, n_hidden[0], 1, 1) - thus, broadcasting\n",
    "        #           to the entire spatial domain\n",
    "\n",
    "        for block in self.blocks[1:]:\n",
    "            embed = block(embed)\n",
    "\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Row_Averaging(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gt,\n",
    "        row_order: int,\n",
    "        grouping: str,\n",
    "        n_T: int,\n",
    "        criterion: nn.Module = nn.MSELoss(),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.gt = gt\n",
    "\n",
    "        if grouping == \"7\":\n",
    "            row_schedule = schedules_7(row_order, n_T, \"rows_t\")\n",
    "        else:\n",
    "            row_schedule = schedules(row_order, n_T, \"rows_t\")\n",
    "\n",
    "        # `register_buffer` will track these tensors for device placement, but\n",
    "        # not store them as model parameters. This is useful for constants.\n",
    "        self.register_buffer(\"rows_t\", row_schedule[\"rows_t\"])\n",
    "        self.rows_t  # Exists! Set by register_buffer\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def degrade(self, x: torch.Tensor, t: int) -> torch.Tensor:\n",
    "        \"\"\"Row averaging diffusion for a set time step\"\"\"\n",
    "\n",
    "        rows_t = self.rows_t[: int(t[0].item())]\n",
    "\n",
    "        z_t = x.clone()\n",
    "\n",
    "        # Average the rows\n",
    "        for i in range(x.shape[0]):\n",
    "            for row in rows_t:\n",
    "                z_t[i, :, row, :] = torch.mean(z_t[i, :, row, :])\n",
    "\n",
    "        return z_t\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Row averaging diffusion\"\"\"\n",
    "\n",
    "        t = torch.randint(1, self.n_T, (x.shape[0],), device=x.device)\n",
    "        z_t = x.clone()\n",
    "        for i, idx in enumerate(t):\n",
    "            rows_t = self.rows_t[:idx]\n",
    "            z_t[i, :, rows_t, :] = torch.mean(\n",
    "                z_t[i, :, rows_t, :], dim=2, keepdim=True\n",
    "            ).expand_as(z_t[i, :, rows_t, :])\n",
    "\n",
    "        # We should predict the \"error term\" from this z_t. Loss is what we return.\n",
    "\n",
    "        return self.criterion(x, self.gt(z_t, t / self.n_T))\n",
    "\n",
    "    def sample(self, n_sample: int, dataset, size, device) -> torch.Tensor:\n",
    "        \"\"\"Algorithm 2 in Bansal et al. (2022)\"\"\"\n",
    "\n",
    "        num_images = len(dataset)\n",
    "        idx = random.sample(range(num_images), n_sample)\n",
    "        z_t = torch.stack([dataset[i][0].clone() for i in idx])\n",
    "        z_t = z_t.to(device)\n",
    "\n",
    "        original = z_t.clone()\n",
    "\n",
    "        T = torch.Tensor([self.n_T])\n",
    "        T = T.expand_as(torch.empty(n_sample))\n",
    "        T = T.to(device)\n",
    "\n",
    "        z_t = self.degrade(z_t, T)\n",
    "\n",
    "        degraded = z_t.clone()\n",
    "\n",
    "        # tensor_values = torch.FloatTensor(n_sample, 1, 28, 28).uniform_(-0.5, -0.2)\n",
    "        # z_t = torch.mean(tensor_values, dim=2, keepdim=True).expand_as(tensor_values)\n",
    "        # z_t = z_t.to(device)\n",
    "\n",
    "        z_t_direct = self.gt(z_t, T / self.n_T)\n",
    "\n",
    "        for t in reversed(range(0, self.n_T)):\n",
    "            if t > 0:\n",
    "                t = torch.Tensor([t])\n",
    "                t = t.expand_as(torch.empty(n_sample))\n",
    "                t = t.to(device)\n",
    "                x_hat = self.gt(z_t, t / self.n_T)\n",
    "                z_t = z_t - self.degrade(x_hat, t) + self.degrade(x_hat, t - 1)\n",
    "\n",
    "            else:\n",
    "                t = torch.Tensor([t])\n",
    "                t = t.expand_as(torch.empty(n_sample))\n",
    "                t = t.to(device)\n",
    "                z_t = self.gt(z_t, t / self.n_T)\n",
    "\n",
    "        return original, degraded, z_t_direct, z_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Col_Averaging(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gt,\n",
    "        col_order: int,\n",
    "        grouping: str,\n",
    "        n_T: int,\n",
    "        criterion: nn.Module = nn.MSELoss(),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.gt = gt\n",
    "\n",
    "        if grouping == \"7\":\n",
    "            col_schedule = schedules_7(col_order, n_T, \"cols_t\")\n",
    "        else:\n",
    "            col_schedule = schedules(col_order, n_T, \"cols_t\")\n",
    "\n",
    "        # `register_buffer` will track these tensors for device placement, but\n",
    "        # not store them as model parameters. This is useful for constants.\n",
    "        self.register_buffer(\"cols_t\", col_schedule[\"cols_t\"])\n",
    "        self.cols_t  # Exists! Set by register_buffer\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def degrade(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Column averaging diffusion for a set time step\"\"\"\n",
    "\n",
    "        # Average the columns\n",
    "        cols_t = self.cols_t[: int(t[0].item())]\n",
    "        z_t = x.clone()\n",
    "        for i in range(x.shape[0]):\n",
    "            for col in cols_t:\n",
    "                z_t[i, :, :, col] = torch.mean(z_t[i, :, :, col])\n",
    "\n",
    "        return z_t\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Column averaging diffusion\"\"\"\n",
    "\n",
    "        t = torch.randint(1, self.n_T, (x.shape[0],), device=x.device)\n",
    "        z_t = x.clone()\n",
    "        for i in range(x.shape[0]):\n",
    "            cols_t = self.cols_t[: t[i]]\n",
    "            for col in cols_t:\n",
    "                z_t[i, :, :, col] = torch.mean(z_t[i, :, :, col])\n",
    "\n",
    "        # We should predict the \"error term\" from this z_t. Loss is what we return.\n",
    "\n",
    "        return self.criterion(x, self.gt(z_t, t / self.n_T))\n",
    "\n",
    "    def sample(self, n_sample: int, dataset, size, device) -> torch.Tensor:\n",
    "        \"\"\"Algorithm 2 in Bansal et al. (2022)\"\"\"\n",
    "\n",
    "        num_images = len(dataset)\n",
    "        idx = random.sample(range(num_images), n_sample)\n",
    "        z_t = torch.stack([dataset[i][0].clone() for i in idx])\n",
    "        z_t = z_t.to(device)\n",
    "        original = z_t.clone()\n",
    "        T = torch.Tensor([self.n_T])\n",
    "        T = T.expand_as(torch.empty(n_sample))\n",
    "        T = T.to(device)\n",
    "\n",
    "        z_t = self.degrade(z_t, T)\n",
    "\n",
    "        z_t_degraded = z_t.clone()\n",
    "        # tensor_values = torch.FloatTensor(n_sample, 1, 28, 28).uniform_(-0.5, -0.2)\n",
    "        # z_t = torch.mean(tensor_values, dim=2, keepdim=True).expand_as(tensor_values)\n",
    "        # z_t = z_t.to(device)\n",
    "\n",
    "        z_t_direct = self.gt(z_t, T / self.n_T)\n",
    "\n",
    "        for t in reversed(range(0, self.n_T)):\n",
    "            if t > 0:\n",
    "                t = torch.Tensor([t])\n",
    "                t = t.expand_as(torch.empty(n_sample))\n",
    "                t = t.to(device)\n",
    "                x_hat = self.gt(z_t, t / self.n_T)\n",
    "                z_t = z_t - self.degrade(x_hat, t) + self.degrade(x_hat, t - 1)\n",
    "\n",
    "            else:\n",
    "                t = torch.Tensor([t])\n",
    "                t = t.expand_as(torch.empty(n_sample))\n",
    "                t = t.to(device)\n",
    "                z_t = self.gt(z_t, t / self.n_T)\n",
    "\n",
    "        return original, z_t_degraded, z_t_direct, z_t\n",
    "\n",
    "    def sample_from_dist(self, n_sample: int, dataset, size, device) -> torch.Tensor:\n",
    "        \"\"\"Algorithm 2 in Bansal et al. (2022)\"\"\"\n",
    "\n",
    "        z_t = torch.empty(n_sample, 1, 28, 28)\n",
    "        z_t[:, :, 0:4, :].uniform_(-0.5, -0.4)\n",
    "        z_t[:, :, 4:8, :].uniform_(-0.5, -0.4)\n",
    "        z_t[:, :, 8:12, :].uniform_(-0.4, -0.2)\n",
    "        z_t[:, :, 12:16, :].uniform_(-0.3, -0.2)\n",
    "        z_t[:, :, 16:20, :].uniform_(-0.4, -0.2)\n",
    "        z_t[:, :, 20:24, :].uniform_(-0.5, -0.4)\n",
    "        z_t[:, :, 24:28, :].uniform_(-0.5, -0.4)\n",
    "        z_t = z_t.to(device)\n",
    "\n",
    "        T = torch.Tensor([self.n_T])\n",
    "        T = T.expand_as(torch.empty(n_sample))\n",
    "        T = T.to(device)\n",
    "\n",
    "        z_t = self.degrade(z_t, T)\n",
    "\n",
    "        z_t_degraded = z_t.clone()\n",
    "        # tensor_values = torch.FloatTensor(n_sample, 1, 28, 28).uniform_(-0.5, -0.2)\n",
    "        # z_t = torch.mean(tensor_values, dim=2, keepdim=True).expand_as(tensor_values)\n",
    "        # z_t = z_t.to(device)\n",
    "\n",
    "        z_t_direct = self.gt(z_t, T / self.n_T)\n",
    "\n",
    "        for t in reversed(range(0, self.n_T)):\n",
    "            if t > 0:\n",
    "                t = torch.Tensor([t])\n",
    "                t = t.expand_as(torch.empty(n_sample))\n",
    "                t = t.to(device)\n",
    "                x_hat = self.gt(z_t, t / self.n_T)\n",
    "                z_t = z_t - self.degrade(x_hat, t) + self.degrade(x_hat, t - 1)\n",
    "\n",
    "            else:\n",
    "                t = torch.Tensor([t])\n",
    "                t = t.expand_as(torch.empty(n_sample))\n",
    "                t = t.to(device)\n",
    "                z_t = self.gt(z_t, t / self.n_T)\n",
    "\n",
    "        return z_t_degraded, z_t_direct, z_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0))])\n",
    "dataset = MNIST(\"./data\", train=True, download=True, transform=tf)\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = CNN(in_channels=1, expected_shape=(28, 28), n_hidden=(16, 32, 32, 16), act=nn.GELU)\n",
    "# For testing: (16, 32, 32, 16)\n",
    "# For more capacity (for example): (64, 128, 256, 128, 64)\n",
    "# dif_model = Col_Averaging(gt=gt, col_order=3, n_T=28, grouping=\"28\")\n",
    "dif_model = Col_Averaging(gt=gt, col_order=3, n_T=7, grouping=\"7\")\n",
    "# dif_model = Row_Averaging(gt=gt, row_order=3, n_T=7, grouping='7')\n",
    "# dif_model = Row_Averaging(gt=gt, row_order=3, n_T=28, grouping='28')\n",
    "optim = torch.optim.Adam(dif_model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "\n",
    "# We wrap our model, optimizer, and dataloaders with `accelerator.prepare`,\n",
    "# which lets HuggingFace's Accelerate handle the device placement and gradient accumulation.\n",
    "dif_model, optim, dataloader = accelerator.prepare(dif_model, optim, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, _ in dataloader:\n",
    "    break\n",
    "\n",
    "with torch.no_grad():\n",
    "    dif_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.0237: 100%|██████████| 468/468 [01:10<00:00,  6.61it/s]\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 1\n",
    "orientation = \"col\"\n",
    "hyper_params = \"default\"\n",
    "losses = []\n",
    "avg_losses = []\n",
    "FID = []\n",
    "IS = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    dif_model.train()\n",
    "\n",
    "    pbar = tqdm(dataloader)  # Wrap our loop with a visual progress bar\n",
    "    for x, _ in pbar:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss = dif_model(x)\n",
    "\n",
    "        loss.backward()\n",
    "        # ^Technically should be `accelerator.backward(loss)` but not necessary for local training\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        avg_loss = np.average(losses[max(len(losses) - 100, 0) :])\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"loss: {avg_loss:.3g}\"\n",
    "        )  # Show running average of loss in progress bar\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "    dif_model.eval()\n",
    "    with torch.no_grad():\n",
    "        original, degraded, direct, xh = dif_model.sample(\n",
    "            16, dataset, (1, 28, 28), accelerator.device\n",
    "        )\n",
    "\n",
    "        for i in range(16):\n",
    "            deg_min = torch.min(degraded[i])\n",
    "            deg_max = torch.max(degraded[i])\n",
    "            degraded[i] = (degraded[i] - deg_min) * (0.5 - (-0.5)) / (\n",
    "                deg_max - deg_min\n",
    "            ) - 0.5\n",
    "        grid = make_grid(xh, nrow=4)\n",
    "        grid1 = make_grid(original, nrow=4)\n",
    "        grid2 = make_grid(degraded, nrow=4)\n",
    "        grid3 = make_grid(direct, nrow=4)\n",
    "\n",
    "        # Evaluate the model using FID and Inception Score\n",
    "        avg_losses.append(avg_loss)\n",
    "        fid_temp = funcs.get_fid(dif_model, dataset, 16, accelerator.device)\n",
    "        is_temp = funcs.get_is_custom(\n",
    "            dif_model, dataset, False, 16, accelerator.device\n",
    "        )[0]\n",
    "        FID.append(fid_temp)\n",
    "        IS.append(is_temp)\n",
    "\n",
    "        # fmt: off\n",
    "        # Save samples to `./contents` directory\n",
    "        if i % 10 == 0:\n",
    "            save_image(grid, f\"./contents/custom_sample_{i:04d}.png\") # noqa E231\n",
    "            save_image(grid1, f\"./contents/original_sample_{i:04d}.png\") # noqa E231\n",
    "            save_image(grid2, f\"./contents/degraded_sample_{i:04d}.png\") # noqa E231\n",
    "            save_image(grid3, f\"./contents/direct_sample_{i:04d}.png\") # noqa E231\n",
    "\n",
    "        # fmt: on\n",
    "\n",
    "        # save model\n",
    "        torch.save(\n",
    "            dif_model.state_dict(),\n",
    "            \"./custom_mnist_\"\n",
    "            + str(n_epoch)\n",
    "            + \"_\"\n",
    "            + orientation\n",
    "            + \"_\"\n",
    "            + hyper_params\n",
    "            + \".pth\",\n",
    "        )  # noqa F541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "degraded, direct, xh = dif_model.sample_from_dist(\n",
    "    1, dataset, (1, 28, 28), accelerator.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16e302960>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = make_grid(xh, nrow=1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M2CW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
